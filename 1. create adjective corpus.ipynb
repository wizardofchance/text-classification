{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION THAT RETURN FILE LENGTH WITHOUT LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrn_file_lg(filename):\n",
    "    \n",
    "    def yld_csv_length(filename):\n",
    "        \n",
    "    # returns one for every yield from file\n",
    "        with open(filename, newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                yield 1\n",
    "    \n",
    "    gen1 = yld_csv_length(filename)\n",
    "    file_lg = 0\n",
    "    for i in gen1:\n",
    "        file_lg += 1\n",
    "\n",
    "    return file_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'df_amazon.csv'\n",
    "\n",
    "file_lg = retrn_file_lg(filename)\n",
    "file_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNTION THAT RETURN ROWS FROM CSV ON DISK WITHIN USER SPECIFIED RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yld_df_rows_csv_on_disk(filename, start, stop, col_idx):    \n",
    "    \n",
    "    def yld_csv_rows(filename):\n",
    "\n",
    "        with open(filename, newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                yield row\n",
    "\n",
    "    raw_corpus = [] \n",
    "    gen_df_rows = yld_csv_rows(filename)\n",
    "    gen_df_rows = itertools.islice(gen_df_rows, start+1, stop+1)\n",
    "\n",
    "    for row in gen_df_rows:\n",
    "        raw_corpus.append(row[col_idx])\n",
    "\n",
    "    \n",
    "    \n",
    "    return raw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE OF CLEANED TEXT CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have bought several the vitality canned dog food products and have found them all good quality the product looks more like stew than processed meat and smells better labrador finicky and she appreciates this product better than most\n",
      "\n",
      "product arrived labeled jumbo salted peanuts the peanuts were actually small sized unsalted not sure this was error the vendor intended represent the product jumbo\n",
      "\n",
      "this confection that has been around few centuries light pillowy citrus gelatin with nuts this case filberts and cut into tiny squares and then liberally coated with powdered sugar and tiny mouthful heaven not too chewy and very flavorful highly recommend this yummy treat you are familiar with the story lewis the lion the witch and the wardrobe this the treat that seduces edmund into selling out his brother and sisters the witch\n",
      "\n",
      "you are looking for the secret ingredient robitussin believe have found got this addition the root beer extract ordered which was good and made some cherry soda the flavor very medicinal\n",
      "\n",
      "great taffy great price there was wide assortment yummy taffy delivery was very quick your taffy lover this deal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename  = 'df_amazon.csv'\n",
    "start, stop = 0, 5\n",
    "col_idx = 3\n",
    "\n",
    "raw_corpus = yld_df_rows_csv_on_disk(filename, start, stop, col_idx)\n",
    "\n",
    "#print(df_amazon_sample.shape)\n",
    "for i in raw_corpus:\n",
    "    print(i)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fn1\n",
    "- YIELD ROWS BETWEEN SPECIFIED RANGE FROM CSV ON DISK AND \n",
    "- FILTER-COLLECT ONLY ADJECTIVES AND IMPORTANT WORDS PER ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrn2_filtered_corpusN(filename, start, stop, bypass = 1):\n",
    "    \n",
    "    from progressbar import ProgressBar    \n",
    "     \n",
    "    def yld_csv_rows(filename):\n",
    "        \n",
    "        # YIELDS ROWS FROM CSV FILE\n",
    "        with open(filename, newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                yield row\n",
    "                \n",
    "    \n",
    "    def retrn_only_correct_spelling(token):\n",
    "        \n",
    "        # RETURNS NULL IF WORD CONTAINS SEQUENCE OF MORE THAN 2 VOWELS OR 3 CONSONANTS \n",
    "        consonants = 'b c d f g h j k l m n p q r s t v w x z'.split()\n",
    "        vowels = 'a e i o u y'.split()\n",
    "        c1 = 0\n",
    "        c2 = 0\n",
    "        for alpha in token.text:\n",
    "            if alpha in vowels:\n",
    "                c1 += 1\n",
    "                c2 = 0\n",
    "                if c1 > 2:\n",
    "                    break\n",
    "            if alpha in consonants:\n",
    "                c1 = 0\n",
    "                c2 += 1\n",
    "                if c2 > 3:\n",
    "                    break\n",
    "        if c1<=2 and c2<=3:\n",
    "            return token\n",
    "        \n",
    "    \n",
    "    def retrn_final_listO_words(doc):\n",
    "        \n",
    "        doc = nlp(doc) #spaCy object\n",
    "        listO_words = []\n",
    "\n",
    "        for token in doc:\n",
    "            \n",
    "            # adding 'listO_words_to_watch' & some words surronding it.\n",
    "            if token.text in listO_words_to_watch:\n",
    "                i = list(doc).index(token)\n",
    "                for n in [-2, -1, 0, 1, 2]:\n",
    "                    try:\n",
    "                        token1 = doc[i+n]\n",
    "                        condition1 = token1.is_stop is False\n",
    "                        condition2 = len(token1.lemma_) >= 3\n",
    "                        condition3 = retrn_only_correct_spelling(token1)\n",
    "                        \n",
    "                        if condition1 and condition2 and condition3:\n",
    "                            listO_words.append(token1.lemma_.lower())\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "            # Futher filtering out stopwords & non adjectives \n",
    "            condition4 = token.pos_ == 'ADJ'\n",
    "            condition5 = retrn_only_correct_spelling(token)\n",
    "\n",
    "            if condition4:  \n",
    "                for m in [-1, 0, 1]:\n",
    "                    try:\n",
    "                        token2 = doc[i+m]\n",
    "                        if token2.is_stop is false and condition5:\n",
    "                            listO_words.append(token2.lemma_.lower())\n",
    "                    except:\n",
    "                        pass                        \n",
    "            listO_words = list(set(listO_words))    \n",
    "        return listO_words\n",
    "    \n",
    "    \n",
    "     # INSTANTIATE GENERATOR:\n",
    "    gen_df_rows = yld_csv_rows(filename)  \n",
    "    \n",
    "    # GENERATE ONLY WITHIN SPECIFIED RANGE:\n",
    "    gen_df_rows = itertools.islice(gen_df_rows, start+bypass, stop+bypass )\n",
    "    \n",
    "    pbar = ProgressBar(max_value=stop-start)\n",
    "    c_pbar, idx = 0, 0\n",
    "    filtered_corpus, indexes = [], [] # ListO_listO_words\n",
    "    \n",
    "    for row in pbar(gen_df_rows):\n",
    "        doc = row[3] # 4th column contains cleanedText\n",
    "        \n",
    "        listO_words = retrn_final_listO_words(doc)\n",
    "        filtered_corpus.append(listO_words)\n",
    "        indexes.append(idx)\n",
    "        idx += 1\n",
    "        c_pbar += 1\n",
    "        pbar.update(c_pbar)\n",
    "    \n",
    "    return filtered_corpus, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fn2\n",
    "- USES Fn1 TO:\n",
    "    - YIELD ROWS BETWEEN SPECIFIED RANGE FROM CSV ON DISK AND \n",
    "    - FILTER-COLLECT ONLY ADJECTIVES AND IMPORTANT WORDS PER ROW\n",
    "- AND THEN SAVES THE FILTERATE AS MULTIPLE EQUALLY SIZED PICKLED FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_filter_corpus_save(filename, start, stop, n_intervals, save_as):\n",
    "    \n",
    "    # USES Fn1 TO YIELD ROWS FROM CSV ON DISK AND FILTER-COLLECT ONLY NOUNS AND VERBS PER SENTENCE,\n",
    "    # AND THEN SAVES FILTERATE AS MULTIPLE EQUALLY SIZED PICKLED FILES\n",
    "    \n",
    "    c1 = 0\n",
    "    c2 =  c1 + 1\n",
    "    n_items =  n_intervals+1\n",
    "    z = [int(i) for i in list(np.linspace(start, stop,  n_items))] \n",
    "\n",
    "    idx = []\n",
    "    while True:\n",
    "        final_corpus, indexes = retrn2_filtered_corpusN(filename, z[c1], z[c2]) # Fn1\n",
    "        idx += indexes\n",
    "        file_name = save_as + str(c2)\n",
    "        np.save(file_name, final_corpus, allow_pickle=True, fix_imports=True) \n",
    "        print(c1+1, \"processed_files saved\")\n",
    "        \n",
    "        c1 += 1\n",
    "        c2 = c1 + 1\n",
    "        if c1 > n_intervals-1:\n",
    "            break\n",
    "            \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING RAW_CORPUS, PROCESSING TO NOUN_VERB BASED CORPUS AND SAVING PROCESSED FILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "listO_words_to_watch = ['not', 'dont', 'yes', 'perfect', 'love', 'loved', 'loves', 'liked', 'likes', 'average', 'inferior', 'expectation', 'expect', \n",
    "                        'disappoint', 'avoid', 'worth', 'never', 'worse', 'loath', 'deceive', 'beautifully', 'wrong', 'missing', \n",
    "                        'concern', 'price', 'cost', 'money','too', 'very', 'fool', 'fooled', 'appreciate', 'again', 'quality', 'amaze', 'wow', \n",
    "                        'class', 'classy', 'niche', 'mistake', 'regret', 'reject', 'yuck', 'yucky', 'ugh', 'lacks','lacking',\n",
    "                        'shit', 'shitty', 'under', 'mouth', 'smell', 'aroma', 'odour', 'flavor', 'taste', 'feel',  'yummy', 'yum', 'super',\n",
    "                        'texture', 'unique', 'fake', 'original', 'bad', 'too', 'really', 'down', 'hit', 'right', 'excellent'\n",
    "                        'favourite', 'crunchy', 'smooth', 'rich', 'soft', 'melt', 'relish', 'verdict', 'stars', 'rate', \n",
    "                        'more', 'deceptive', 'deceive', 'back', 'again', 'old','spoilt',\n",
    "                        'sad', 'unhappy', \"was'nt\", 'good', 'aweful', 'beware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:31:57 Time:  0:31:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processed_files saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:31:05 Time:  0:31:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 processed_files saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:30:38 Time:  0:30:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 processed_files saved\n",
      "Wall time: 1h 33min 43s\n"
     ]
    }
   ],
   "source": [
    "filename = 'df_amazon.csv'\n",
    "save_as = 'corpus_ADJECTIVE_amazon_0_to_150_000_'\n",
    "start = 0\n",
    "stop = 150_000\n",
    "n_intervals = 3\n",
    "\n",
    "\n",
    "%time idx = fn_filter_corpus_save(filename, start, stop, n_intervals, save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:26:17 Time:  0:26:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processed_files saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:29:13 Time:  0:29:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 processed_files saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000 of 50000) |##################| Elapsed Time: 0:24:56 Time:  0:24:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 processed_files saved\n",
      "Wall time: 1h 20min 32s\n"
     ]
    }
   ],
   "source": [
    "filename = 'df_amazon.csv'\n",
    "save_as = 'corpus_ADJECTIVE_150_000_to_300_000'\n",
    "start = 150_000\n",
    "stop = 300_000\n",
    "n_intervals = 3\n",
    "\n",
    "\n",
    "%time idx = fn_filter_corpus_save(filename, start, stop, n_intervals, save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (32060 of 32060) |##################| Elapsed Time: 0:16:26 Time:  0:16:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processed_files saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (32060 of 32060) |##################| Elapsed Time: 0:15:56 Time:  0:15:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 processed_files saved\n",
      "Wall time: 32min 28s\n"
     ]
    }
   ],
   "source": [
    "filename = 'df_amazon.csv'\n",
    "save_as = 'corpus_ADJECTIVE_300_000_to_364120_'\n",
    "start = 300_000\n",
    "stop = file_lg\n",
    "n_intervals = 2\n",
    "\n",
    "\n",
    "%time idx = fn_filter_corpus_save(filename, start, stop, n_intervals, save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64119"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERTING PROCESSSED FILES INTO DATAFRAMES AND SAVING AS CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364119"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1 = list(np.load('corpus_ADJECTIVE_amazon_0_to_150_000_1.npy'))\n",
    "corpus2 = list(np.load('corpus_ADJECTIVE_amazon_0_to_150_000_2.npy'))\n",
    "corpus3 = list(np.load('corpus_ADJECTIVE_amazon_0_to_150_000_3.npy'))\n",
    "corpus4 = list(np.load('corpus_ADJECTIVE_150_000_to_300_000_1.npy'))\n",
    "corpus5 = list(np.load('corpus_ADJECTIVE_150_000_to_300_000_2.npy'))\n",
    "corpus6 = list(np.load('corpus_ADJECTIVE_150_000_to_300_000_3.npy'))\n",
    "corpus7 = list(np.load('corpus_ADJECTIVE_300_000_to_364120_1.npy'))\n",
    "corpus8 = list(np.load('corpus_ADJECTIVE_300_000_to_364120_2.npy'))\n",
    "\n",
    "corpus = corpus1 + corpus2 + corpus3 + corpus4 + corpus5 + corpus6 + corpus7 + corpus8\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS DOCS AS LISTO_WORDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364119"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename  = 'df_amazon.csv'\n",
    "start, stop = 0, len(df_amazon)+1\n",
    "col_idx = 2\n",
    "\n",
    "score = yld_df_rows_csv_on_disk(filename, start, stop, col_idx)\n",
    "\n",
    "len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative', 'positive', 'negative', 'positive']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364119, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listO_words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[good, stew, look, product, like, quality]</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unsalted, sized, sure]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[recommend, chewy, heaven, highly, yummy, mout...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[good, medicinal, soda, flavor]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[price, assortment, yummy, delivery, great, ta...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         listO_words     score\n",
       "0         [good, stew, look, product, like, quality]  positive\n",
       "1                            [unsalted, sized, sure]  negative\n",
       "2  [recommend, chewy, heaven, highly, yummy, mout...  positive\n",
       "3                    [good, medicinal, soda, flavor]  negative\n",
       "4  [price, assortment, yummy, delivery, great, ta...  positive"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adjective_listO_words = pd.DataFrame(np.array(corpus))\n",
    "\n",
    "print(df_adjective_listO_words.shape)\n",
    "\n",
    "df_adjective_listO_words.columns = ['listO_words']\n",
    "df_adjective_listO_words = df_adjective_listO_words.assign(score = score)\n",
    "df_adjective_listO_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('df_adjective_listO_words', df_adjective_listO_words.listO_words.values, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS DOCS AS TEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrn2_text_from_listO_words(corpus):\n",
    "    \n",
    "    adj_text = []\n",
    "    anomaly = []\n",
    "    c = 0\n",
    "    for listO_words in corpus:\n",
    "        if len(listO_words) > 0:\n",
    "            text = ' '.join(listO_words)\n",
    "            adj_text.append(text)\n",
    "        else:\n",
    "            adj_text.append('a')\n",
    "            anomaly.append(c)\n",
    "        c += 1  \n",
    "    return adj_text, anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364119, 29381)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus\n",
    "\n",
    "corpus_text, anomaly = retrn2_text_from_listO_words(corpus)\n",
    "len(corpus_text), len(anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 20, 25, 81, 88]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364119, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good stew look product like quality</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unsalted sized sure</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recommend chewy heaven highly yummy mouthful f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good medicinal soda flavor</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>price assortment yummy delivery great taffy wi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     score\n",
       "0                good stew look product like quality  positive\n",
       "1                                unsalted sized sure  negative\n",
       "2  recommend chewy heaven highly yummy mouthful f...  positive\n",
       "3                         good medicinal soda flavor  negative\n",
       "4  price assortment yummy delivery great taffy wi...  positive"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adjective_text = pd.DataFrame(corpus_text)\n",
    "df_adjective_text.columns = ['text']\n",
    "df_adjective_text = df_adjective_text.assign(score = score)#.drop(anomaly)\n",
    "\n",
    "\n",
    "print(df_adjective_text.shape)\n",
    "df_adjective_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjective_text.to_csv('df_adjective_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
